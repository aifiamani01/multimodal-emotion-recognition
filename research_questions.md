1. Can multimodal representations (video + facial + audio) improve emotion recognition?
2. Do facial cues or audio cues dominate in certain emotions?
3. How does adding temporal context (video frames over time) affect predictions?
4. Can we probe frozen pretrained models for multimodal embeddings before fine-tuning?
5. What are the failure cases, e.g., noisy audio or occluded faces?
